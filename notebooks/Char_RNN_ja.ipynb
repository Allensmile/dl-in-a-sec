{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックは、以下のノートブックを元に日本語訳、一部章立ての再構成、加筆を行いました。\n",
    "https://colab.research.google.com/drive/13Vr3PrDg7cc4OZ3W2-grLSVSf0RJYWzb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jD40N62ix6k_"
   },
   "source": [
    "\n",
    "# Char-RNNのTensorFlow実装\n",
    "\n",
    " CharRNNは、Andrej Karpathy氏によって作成された、有名なテキスト生成モデル（文字レベルLSTM）です。任意のテキストで簡単に学習と生成を試すことができます。以下のような、面白い出力例が知られています： \n",
    "\n",
    "  * Music: abc notation 音楽のドレミ(ABC)表記\n",
    "<https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/>,\n",
    "  * Irish folk music アイルランド伝統音楽\n",
    "<https://soundcloud.com/seaandsailor/sets/char-rnn-composes-irish-folk-music>-\n",
    "  * Obama speeches オバマ元大統領のスピーチ\n",
    "<https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0>-\n",
    "  * Eminem lyrics エミネムの歌詞\n",
    "<https://soundcloud.com/mrchrisjohnson/recurrent-neural-shady>- (NSFW ;-))\n",
    "  * Research awards 研究のアワード\n",
    "<http://karpathy.github.io/2015/05/21/rnn-effectiveness/#comment-2073825449>-\n",
    "  * TED Talks TEDトーク\n",
    "<https://medium.com/@samim/ted-rnn-machine-generated-ted-talks-3dd682b894c0>-\n",
    "  * Movie Titles 映画のタイトル <http://www.cs.toronto.edu/~graves/handwriting.html>\n",
    "\n",
    "このノートブックはTensorFlowでの再実装です。generatorに模倣させたいテキストを入力し、モデルを学習させ、結果を確認し、今後のために学習済みモデルを保存する、という一連の流れを試します。 \n",
    "\n",
    "各ステップの指示に従って、セルを順番に実行するだけで始められます。アップロードの指示があったら、サイズの大きい（少なくとも1MB以上）テキストファイルが必要です。無くても心配いりません。試しにシェイクスピアの作品から成る準備済みのテキストコーパスを使えます。\n",
    "\n",
    "学習セルは30秒ごとにチェックポイントを保存します。また、ネットワークの出力をチェックでき、途中から学習を再開することもできます。\n",
    "\n",
    "## 概要\n",
    "\n",
    "このノートブックは以下のステップで進めます: \n",
    "\n",
    "- データをアップロードする\n",
    "- ハイパーパラメータを設定する（デフォルト値も使えます） \n",
    "- モデル、学習時の損失関数、およびデータ入力マネージャを定義する \n",
    "- クラウドGPUを使って、モデルを学習する\n",
    "- モデルを保存し、それを使って新しいテキストを生成する\n",
    "\n",
    "\n",
    "RNNの設計は、Andrej Karpathy氏の[char-rnn](https://github.com/karpathy/char-rnn)を基にした[このgithubプロジェクト](https://github.com/sherjilozair/char-rnn-tensorflow)を参考にしました。より詳細を知りたい場合は、Andrej氏の[ブログ投稿](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)を読むことから始めるとよいでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZ0X3Nizx6lA"
   },
   "source": [
    "\n",
    "## A. 環境を準備する\n",
    "\n",
    "コードの実行に必要なインポートと、定数の定義を行います。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BM5JcMiKx6lC"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division\n",
    "from google.colab import files\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "CHECKPOINT_DIR = './checkpoints/'  #Checkpoints are temporarily kept here.\n",
    "TEXT_ENCODING = 'utf-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBvGcr1jx6lF"
   },
   "source": [
    "\n",
    "## B. データセットを準備する\n",
    "\n",
    "シェイクスピアの作品群をダウンロードして、学習データセットとして使えます。\n",
    "\n",
    "自身で準備した任意のテキストファイルを使いたい場合は、それをアップロードし、学習データセットとして使えます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. データセットをダウンロードする\n",
    "\n",
    "デフォルトでは、シェイクスピアの作品群をデータセットとして使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VlmglAYx6lG",
    "outputId": "9dcb32f3-3e3a-457d-b713-e3c7a3a05e14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An excerpt: \n",
      "                      1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n"
     ]
    }
   ],
   "source": [
    "shakespeare_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
    "import urllib\n",
    "file_contents = urllib.urlopen(shakespeare_url).read()\n",
    "file_name = \"shakespeare\"\n",
    "file_contents = file_contents[10501:]  # Skip headers and start at content\n",
    "print(\"An excerpt: \\n\", file_contents[:664])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (オプション）自分で準備したデータセットを使う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eH5UaL5sx6lL"
   },
   "source": [
    "\n",
    "自身で用意した学習データセットを使う場合は、次の2つのセルを実行します。使わない場合は、スキップしてください。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lWNbHkhCx6lM",
    "outputId": "f5b04f78-b1e5-4e00-d631-1b152e7f4419"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-3a809afd-41fd-4ee6-8505-e4679e54c4b2\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-3a809afd-41fd-4ee6-8505-e4679e54c4b2\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3oNVnsmx6lP",
    "outputId": "722b07ce-d077-4a90-b73d-78c90a5f47df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An excerpt: \n",
      "                      1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n"
     ]
    }
   ],
   "source": [
    "if uploaded:\n",
    "  if type(uploaded) is not dict: uploaded = uploaded.files  ## Deal with filedit versions\n",
    "  file_bytes = uploaded[uploaded.keys()[0]]\n",
    "  utf8_string = file_bytes.decode(TEXT_ENCODING)\n",
    "  file_contents = utf8_string if files else ''\n",
    "  file_name = uploaded.keys()[0]\n",
    "print(\"An excerpt: \\n\", file_contents[:664])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. データセットを前処理する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "まず、プレーンテキストのファイルを、トークンの配列に変換します。そのために、このトークンマッパーヘルパークラスを使用します。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "class TokenMapper(object):\n",
    "  def __init__(self):\n",
    "    self.token_mapping = {}\n",
    "    self.reverse_token_mapping = {}\n",
    "  def buildFromData(self, utf8_string, limit=0.00004):\n",
    "    print(\"Build token dictionary.\")\n",
    "    total_num = len(utf8_string)\n",
    "    sorted_tokens = sorted(Counter(utf8_string.decode('utf8')).items(), \n",
    "                           key=lambda x: -x[1])\n",
    "    # Filter tokens: Only allow printable characters (not control chars) and\n",
    "    # limit to ones that are resonably common, i.e. skip strange esoteric \n",
    "    # characters in order to reduce the dictionary size.\n",
    "    filtered_tokens = filter(lambda t: t[0] in string.printable or \n",
    "                             float(t[1])/total_num > limit, sorted_tokens)\n",
    "    tokens, counts = zip(*filtered_tokens)\n",
    "    self.token_mapping = dict(zip(tokens, range(len(tokens))))\n",
    "    for c in string.printable:\n",
    "      if c not in self.token_mapping:\n",
    "        print(\"Skipped token for: \", c)\n",
    "    self.reverse_token_mapping = {\n",
    "        val: key for key, val in self.token_mapping.items()}\n",
    "    print(\"Created dictionary: %d tokens\"%len(self.token_mapping))\n",
    "  \n",
    "  def mapchar(self, char):\n",
    "    if char in self.token_mapping:\n",
    "      return self.token_mapping[char]\n",
    "    else:\n",
    "      return self.token_mapping[' ']\n",
    "  \n",
    "  def mapstring(self, utf8_string):\n",
    "    return [self.mapchar(c) for c in utf8_string]\n",
    "  \n",
    "  def maptoken(self, token):\n",
    "    return self.reverse_token_mapping[token]\n",
    "  \n",
    "  def maptokens(self, int_array):\n",
    "    return ''.join([self.reverse_token_mapping[c] for c in int_array])\n",
    "  \n",
    "  def size(self):\n",
    "    return len(self.token_mapping)\n",
    "  \n",
    "  def alphabet(self):\n",
    "    return ''.join([k for k,v in sorted(self.token_mapping.items(),key=itemgetter(1))])\n",
    "\n",
    "  def print(self):\n",
    "    for k,v in sorted(self.token_mapping.items(),key=itemgetter(1)): print(k, v)\n",
    "  \n",
    "  def save(self, path):\n",
    "    with open(path, 'wb') as json_file:\n",
    "      json.dump(self.token_mapping, json_file)\n",
    "  \n",
    "  def restore(self, path):\n",
    "    with open(path, 'r') as json_file:\n",
    "      self.token_mapping = {}\n",
    "      self.token_mapping.update(json.load(json_file))\n",
    "      self.reverse_token_mapping = {val: key for key, val in self.token_mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生の入力をトークンのリストに変換しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12416\n",
      "drwxr-xr-x  2 tomo.masuda  staff       64  5  6 15:55 \u001b[34m{CHECKPOINT_DIR}\u001b[m\u001b[m\n",
      "-rw-r--r--  1 tomo.masuda  staff    49828  5  6 15:53 Char_RNN_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    14926  5  6 15:46 TF_Hub_Universal_Encoder_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    23647  5  6 15:43 Text_classification_with_TF_Hub_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    95773  5  6 12:34 Basic_Text_Classification_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff   108791  5  6 12:27 BigGAN_TF_Hub_Demo_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    15742  5  6 11:48 Compare_GAN_ja.ipynb\n",
      "-rwxr-xr-x@ 1 tomo.masuda  staff    13053  5  6 11:42 \u001b[31mAction_Recognition_on_the_UCF101_Dataset_ja_only.ipynb\u001b[m\u001b[m\n",
      "-rw-r--r--  1 tomo.masuda  staff   777239  5  6 11:41 Deepdream_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    14102  5  6 11:24 TF_Hub_Delf_module_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff   674878  5  6 11:05 Transfer_Learning_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    31694  5  6 10:57 Basic_Classification_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    43999  5  6 02:19 MusicVAE_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff  2264318  5  5 12:32 CycleGAN_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    10958  5  2 11:02 TF_Hub_Action_Recognition_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    60159  5  1 18:32 Onsets_and_Frames_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    58061  5  1 18:30 Performance_RNN_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff   185145  5  1 18:18 NSynth_Colab_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff  1844734  5  1 17:59 Image_Captioning_ja.ipynb\n",
      "-rw-r--r--  1 tomo.masuda  staff    34145  5  1 17:19 NMT_with_Attention_ja.ipynb\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sequence_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2a1a077eadbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls -lt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mchars_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfile_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0munique_sequential_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mchars_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequence_length' is not defined"
     ]
    }
   ],
   "source": [
    "# Clean the checkpoint directory and make a fresh one\n",
    "!rm -rf {CHECKPOINT_DIR}\n",
    "!mkdir {CHECKPOINT_DIR}\n",
    "!ls -lt\n",
    "\n",
    "chars_in_batch = (sequence_length * batch_size)\n",
    "file_len = len(file_contents)\n",
    "unique_sequential_batches = file_len // chars_in_batch\n",
    "\n",
    "mapper = TokenMapper()\n",
    "mapper.buildFromData(file_contents)\n",
    "mapper.save(''.join([CHECKPOINT_DIR, 'token_mapping.json']))\n",
    "\n",
    "input_values = mapper.mapstring(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MG_M4cK4x6lS"
   },
   "source": [
    "\n",
    "## D. モデルを作成する\n",
    "\n",
    "### 1. LSTMモデルを作る\n",
    "\n",
    "まずはじめに、ニューラルネットワークの構造を決める必要があります。次のセルでは、ネットワークを構成するTensorFlowグラフと学習時のハイパーパラメータを含むクラスを作ります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TgxABsYx6lS"
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "  \"\"\"Represents a Recurrent Neural Network using LSTM cells.\n",
    "\n",
    "  Attributes:\n",
    "    num_layers: The integer number of hidden layers in the RNN.\n",
    "    state_size: The size of the state in each LSTM cell.\n",
    "    num_classes: Number of output classes. (E.g. 256 for Extended ASCII).\n",
    "    batch_size: The number of training sequences to process per step.\n",
    "    sequence_length: The number of chars in a training sequence.\n",
    "    batch_index: Index within the dataset to start the next batch at.\n",
    "    on_gpu_sequences: Generates the training inputs for a single batch.\n",
    "    on_gpu_targets: Generates the training labels for a single batch.\n",
    "    input_symbol: Placeholder for a single label for use during inference.\n",
    "    temperature: Used when sampling outputs. A higher temperature will yield\n",
    "      more variance; a lower one will produce the most likely outputs. Value\n",
    "      should be between 0 and 1.\n",
    "    initial_state: The LSTM State Tuple to initialize the network with. This\n",
    "      will need to be set to the new_state computed by the network each cycle.\n",
    "    logits: Unnormalized probability distribution for the next predicted\n",
    "      label, for each timestep in each sequence.\n",
    "    output_labels: A [batch_size, 1] int32 tensor containing a predicted\n",
    "      label for each sequence in a batch. Only generated in infer mode.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               rnn_num_layers=1,\n",
    "               rnn_state_size=128,\n",
    "               num_classes=256,\n",
    "               rnn_batch_size=1,\n",
    "               rnn_sequence_length=1):\n",
    "    self.num_layers = rnn_num_layers\n",
    "    self.state_size = rnn_state_size\n",
    "    self.num_classes = num_classes\n",
    "    self.batch_size = rnn_batch_size\n",
    "    self.sequence_length = rnn_sequence_length\n",
    "    self.batch_shape = (self.batch_size, self.sequence_length)\n",
    "    print(\"Built LSTM: \",\n",
    "          self.num_layers ,self.state_size ,self.num_classes ,\n",
    "          self.batch_size ,self.sequence_length ,self.batch_shape)\n",
    "\n",
    "\n",
    "  def build_training_model(self, dropout_rate, data_to_load):\n",
    "    \"\"\"Sets up an RNN model for running a training job.\n",
    "\n",
    "    Args:\n",
    "      dropout_rate: The rate at which weights may be forgotten during training.\n",
    "      data_to_load: A numpy array of containing the training data, with each\n",
    "        element in data_to_load being an integer representing a label. For\n",
    "        example, for Extended ASCII, values may be 0 through 255.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If mode is data_to_load is None.\n",
    "    \"\"\"\n",
    "    if data_to_load is None:\n",
    "      raise ValueError('To continue, you must upload training data.')\n",
    "    inputs = self._set_up_training_inputs(data_to_load)\n",
    "    self._build_rnn(inputs, dropout_rate)\n",
    "\n",
    "  def build_inference_model(self):\n",
    "    \"\"\"Sets up an RNN model for generating a sequence element by element.\n",
    "    \"\"\"\n",
    "    self.input_symbol = tf.placeholder(shape=[1, 1], dtype=tf.int32)\n",
    "    self.temperature = tf.placeholder(shape=(), dtype=tf.float32,\n",
    "                                      name='temperature')\n",
    "    self.num_options = tf.placeholder(shape=(), dtype=tf.int32,\n",
    "                                      name='num_options')\n",
    "    self._build_rnn(self.input_symbol, 0.0)\n",
    "\n",
    "    self.temperature_modified_logits = tf.squeeze(\n",
    "        self.logits, 0) / self.temperature\n",
    "\n",
    "    #for beam search\n",
    "    self.normalized_probs = tf.nn.softmax(self.logits)\n",
    "\n",
    "    self.output_labels = tf.multinomial(self.temperature_modified_logits,\n",
    "                                        self.num_options)\n",
    "\n",
    "  def _set_up_training_inputs(self, data):\n",
    "    self.batch_index = tf.placeholder(shape=(), dtype=tf.int32)\n",
    "    batch_input_length = self.batch_size * self.sequence_length\n",
    "\n",
    "    input_window = tf.slice(tf.constant(data, dtype=tf.int32),\n",
    "                            [self.batch_index],\n",
    "                            [batch_input_length + 1])\n",
    "\n",
    "    self.on_gpu_sequences = tf.reshape(\n",
    "        tf.slice(input_window, [0], [batch_input_length]), self.batch_shape)\n",
    "\n",
    "    self.on_gpu_targets = tf.reshape(\n",
    "        tf.slice(input_window, [1], [batch_input_length]), self.batch_shape)\n",
    "\n",
    "    return self.on_gpu_sequences\n",
    "\n",
    "  def _build_rnn(self, inputs, dropout_rate):\n",
    "    \"\"\"Generates an RNN model using the passed functions.\n",
    "\n",
    "    Args:\n",
    "      inputs: int32 Tensor with shape [batch_size, sequence_length] containing\n",
    "        input labels.\n",
    "      dropout_rate: A floating point value determining the chance that a weight\n",
    "        is forgotten during evaluation.\n",
    "    \"\"\"\n",
    "    # Alias some commonly used functions\n",
    "    dropout_wrapper = tf.contrib.rnn.DropoutWrapper\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell\n",
    "    multi_rnn_cell = tf.contrib.rnn.MultiRNNCell\n",
    "\n",
    "    self._cell = multi_rnn_cell(\n",
    "        [dropout_wrapper(lstm_cell(self.state_size), 1.0, 1.0 - dropout_rate)\n",
    "         for _ in range(self.num_layers)])\n",
    "\n",
    "    self.initial_state = self._cell.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "    embedding = tf.get_variable('embedding',\n",
    "                                [self.num_classes, self.state_size])\n",
    "\n",
    "    embedding_input = tf.nn.embedding_lookup(embedding, inputs)\n",
    "    output, self.new_state = tf.nn.dynamic_rnn(self._cell, embedding_input,\n",
    "                                               initial_state=self.initial_state)\n",
    "\n",
    "    self.logits = tf.contrib.layers.fully_connected(output, self.num_classes,\n",
    "                                                    activation_fn=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1oxq_THx6lW"
   },
   "source": [
    "\n",
    "### 2. ハイパーパラメータを定義する\n",
    "\n",
    "これらの学習時のハイパーパラメータを決めます。さらに推論時は、テキスト生成用のパラメータを定義します。 \n",
    "\n",
    "まずはデフォルト値のまま、このセルを実行してください。後ほど、このセルに戻って、パラメータのチューニングを試せます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nixrXR4x6lW"
   },
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "state_size = 256\n",
    "batch_size = 64\n",
    "sequence_length = 256\n",
    "num_training_steps = 30000 # Takes about 40 minuets \n",
    "steps_per_epoch = 500\n",
    "learning_rate = 0.002\n",
    "learning_rate_decay = 0.95\n",
    "gradient_clipping = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFSAYIXsx6lZ"
   },
   "source": [
    "\n",
    "### 3. 損失関数を定義する\n",
    "\n",
    "誤差(loss)は、ニューラルネットワークモデルが、データ分布をどの程度良くモデリングできているかの尺度です。 \n",
    "\n",
    "モデルの出力である`logit`と、学習対象の`target`を渡します。この場合、`target_weights`への重み付けですが、このノートブックでは、すべて1として偏りは与えません。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mi15zCVCx6lZ"
   },
   "outputs": [],
   "source": [
    "def get_loss(logits, targets, target_weights):\n",
    "  with tf.name_scope('loss'):\n",
    "    return tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        target_weights,\n",
    "        average_across_timesteps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxzZJ5nDx6ld"
   },
   "source": [
    "\n",
    "### 4. オプティマイザを定義する\n",
    "\n",
    "これはTensorFlowに損失を減らしていく、最適化の手法を指定します。一般的な[ADAMアルゴリズム](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)を使います。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7FwrEAqx6ld"
   },
   "outputs": [],
   "source": [
    "def get_optimizer(loss, initial_learning_rate, gradient_clipping, global_step,\n",
    "                  decay_steps, decay_rate):\n",
    "\n",
    "  with tf.name_scope('optimizer'):\n",
    "    computed_learning_rate = tf.train.exponential_decay(\n",
    "        initial_learning_rate,\n",
    "        global_step,\n",
    "        decay_steps,\n",
    "        decay_rate,\n",
    "        staircase=True)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(computed_learning_rate)\n",
    "    trained_vars = tf.trainable_variables()\n",
    "    gradients, _ = tf.clip_by_global_norm(\n",
    "        tf.gradients(loss, trained_vars),\n",
    "        gradient_clipping)\n",
    "    training_op = optimizer.apply_gradients(\n",
    "        zip(gradients, trained_vars),\n",
    "        global_step=global_step)\n",
    "\n",
    "    return training_op, computed_learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_grjmEnpx6lg"
   },
   "source": [
    "\n",
    "### 5. 学習進捗確認の関数を定義する\n",
    "\n",
    "このクラスでは、学習の進行状況を確認できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8jSoI20x6lg"
   },
   "outputs": [],
   "source": [
    "class LossPlotter(object):\n",
    "  def __init__(self, history_length):\n",
    "    self.global_steps = []\n",
    "    self.losses = []\n",
    "    self.averaged_loss_x = []\n",
    "    self.averaged_loss_y = []\n",
    "    self.history_length = history_length\n",
    "\n",
    "  def draw_plots(self):\n",
    "    self._update_averages(self.global_steps, self.losses,\n",
    "                          self.averaged_loss_x, self.averaged_loss_y)\n",
    "\n",
    "    plt.title('Average Loss Over Time')\n",
    "    plt.xlabel('Global Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(self.averaged_loss_x, self.averaged_loss_y, label='Loss/Time (Avg)')\n",
    "    plt.plot()\n",
    "    plt.plot(self.global_steps, self.losses,\n",
    "             label='Loss/Time (Last %d)' % self.history_length,\n",
    "             alpha=.1, color='r')\n",
    "    plt.plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Loss for the last 100 Steps')\n",
    "    plt.xlabel('Global Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(self.global_steps, self.losses,\n",
    "             label='Loss/Time (Last %d)' % self.history_length, color='r')\n",
    "    plt.plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # The notebook will be slowed down at the end of training if we plot the\n",
    "    # entire history of raw data. Plot only the last 100 steps of raw data,\n",
    "    # and the average of each 100 batches. Don't keep unused data.\n",
    "    self.global_steps = []\n",
    "    self.losses = []\n",
    "    self.learning_rates = []\n",
    "\n",
    "  def log_step(self, global_step, loss):\n",
    "    self.global_steps.append(global_step)\n",
    "    self.losses.append(loss)\n",
    "\n",
    "  def _update_averages(self, x_list, y_list,\n",
    "                       averaged_data_x, averaged_data_y):\n",
    "    averaged_data_x.append(x_list[-1])\n",
    "    averaged_data_y.append(sum(y_list) / self.history_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qyQDsiOSx6lk"
   },
   "source": [
    "\n",
    "モデルの学習にはしばらく時間がかかります。コーヒーを飲みながら待っても良いでしょう。学習の30秒ごとに、進捗を失わぬよう、チェックポイントを保存します。トレーニングの進行状況を見るには、時々学習を中止し、推論セルを実行し、学習中のモデルでテキストを生成してみましょう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxV2RrfEx6lu"
   },
   "source": [
    "\n",
    "### 6. モデルを作成する\n",
    "\n",
    "モデルを定義し、Tensorflowグラフに学習操作を追加します。 \n",
    "\n",
    "generatorのテスト後も学習を続けている場合は、次の3つのセルを実行してください。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUX_OnvMx6lu",
    "outputId": "36f5da9f-47ad-4eb6-dcb6-64019fd74a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing model...\n",
      "Built LSTM:  2 256 84 64 256 (64, 256)\n",
      "Constructed model successfully.\n",
      "Setting up training session...\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "print('Constructing model...')\n",
    "\n",
    "model = RNN(\n",
    "    rnn_num_layers=num_layers,\n",
    "    rnn_state_size=state_size,\n",
    "    num_classes=mapper.size(),\n",
    "    rnn_batch_size=batch_size,\n",
    "    rnn_sequence_length=sequence_length)\n",
    "\n",
    "model.build_training_model(0.05, np.asarray(input_values))\n",
    "print('Constructed model successfully.')\n",
    "\n",
    "print('Setting up training session...')\n",
    "neutral_target_weights = tf.constant(\n",
    "    np.ones(model.batch_shape),\n",
    "    tf.float32\n",
    ")\n",
    "loss = get_loss(model.logits, model.on_gpu_targets, neutral_target_weights)\n",
    "global_step = tf.get_variable('global_step', shape=(), trainable=False,\n",
    "                              dtype=tf.int32)\n",
    "training_step, computed_learning_rate = get_optimizer(\n",
    "    loss,\n",
    "    learning_rate,\n",
    "    gradient_clipping,\n",
    "    global_step,\n",
    "    steps_per_epoch,\n",
    "    learning_rate_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDWMogKUx6ly"
   },
   "source": [
    "\n",
    "`Supervisor` は、学習の流れとチェックポイントを管理します。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6nlucXGx6lz",
    "outputId": "5432852b-556d-41f1-ff4e-cd22877cc162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training session ready.\n"
     ]
    }
   ],
   "source": [
    "# Create a supervisor that will checkpoint the model in the CHECKPOINT_DIR\n",
    "sv = tf.train.Supervisor(\n",
    "    logdir=CHECKPOINT_DIR,\n",
    "    global_step=global_step,\n",
    "    save_model_secs=30)\n",
    "print('Training session ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHeUJki7x6l2"
   },
   "source": [
    "###This next cell will begin the training cycle. \n",
    "First, we will attempt to pick up training where we left off, if a previous checkpoint exists, then continue the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r49sZrH3x6l3"
   },
   "source": [
    "\n",
    "## E. モデルを学習させる\n",
    "\n",
    "この次のセルは学習サイクルを始めます。 以前のチェックポイントが保存されていたら、中断したところから学習を再開しようとします。学習プロセスを続けます。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtnjdMAbx6l3"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "with sv.managed_session(config=config) as sess:\n",
    "  print('Training supervisor successfully initialized all variables.')\n",
    "  if not file_len:\n",
    "    raise ValueError('To continue, you must upload training data.')\n",
    "  elif file_len < chars_in_batch:\n",
    "    raise ValueError('To continue, you must upload a larger set of data.')\n",
    "\n",
    "  plotter = LossPlotter(100)\n",
    "  step_number = sess.run(global_step)\n",
    "  zero_state = sess.run([model.initial_state])\n",
    "  max_batch_index = (unique_sequential_batches - 1) * chars_in_batch\n",
    "  while not sv.should_stop() and step_number < num_training_steps:\n",
    "    feed_dict = {\n",
    "        model.batch_index: randint(0, max_batch_index),\n",
    "        model.initial_state: zero_state\n",
    "        }\n",
    "    [_, _, training_loss, step_number, current_learning_rate, _] = sess.run(\n",
    "        [model.on_gpu_sequences,\n",
    "         model.on_gpu_targets,\n",
    "         loss,\n",
    "         global_step,\n",
    "         computed_learning_rate,\n",
    "         training_step],\n",
    "        feed_dict)\n",
    "    plotter.log_step(step_number, training_loss)\n",
    "    if step_number % 100 == 0:\n",
    "      clear_output(True)\n",
    "      plotter.draw_plots()\n",
    "      print('Latest checkpoint is: %s' %\n",
    "            tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
    "      print('Learning Rate is: %f' %\n",
    "            current_learning_rate)\n",
    "\n",
    "    if step_number % 10 == 0:\n",
    "      print('global step %d, loss=%f' % (step_number, training_loss))\n",
    "\n",
    "clear_output(True)\n",
    "\n",
    "print('Training completed in HH:MM:SS = ', datetime.now()-start_time)\n",
    "print('Latest checkpoint is: %s' %\n",
    "      tf.train.latest_checkpoint(CHECKPOINT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJ3XaNblx6l6"
   },
   "source": [
    "\n",
    "## F. 学習済みモデルを評価する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. テキストを生成する\n",
    "\n",
    "いよいよ、テキストを生成してみましょう！ここでは、 **ビームサーチ**アルゴリズムと学習済みモデルを使って、テキストを生成します。ビームサーチは、各ステップで現在の各オプションからN個の次のオプションを候補として挙げます。generatorが「筋の悪い」項目を選択しても、その悪い結果を無視して、より可能性の高い選択肢をとり続けることができます。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WABwSrMax6l6"
   },
   "outputs": [],
   "source": [
    "class BeamSearchCandidate(object):\n",
    "  \"\"\"Represents a node within the search space during Beam Search.\n",
    "\n",
    "  Attributes:\n",
    "    state: The resulting RNN state after the given sequence has been generated.\n",
    "    sequence: The sequence of selections leading to this node.\n",
    "    probability: The probability of the sequence occurring, computed as the sum\n",
    "      of the probabilty of each character in the sequence at its respective\n",
    "      step.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, init_state, sequence, probability):\n",
    "    self.state = init_state\n",
    "    self.sequence = sequence\n",
    "    self.probability = probability\n",
    "\n",
    "  def search_from(self, tf_sess, rnn_model, temperature, num_options):\n",
    "    \"\"\"Expands the num_options most likely next elements in the sequence.\n",
    "\n",
    "    Args:\n",
    "      tf_sess: The Tensorflow session containing the rnn_model.\n",
    "      rnn_model: The RNN to use to generate the next element in the sequence.\n",
    "      temperature: Modifies the probabilities of each character, placing\n",
    "        more emphasis on higher probabilities as the value approaches 0.\n",
    "      num_options: How many potential next options to expand from this one.\n",
    "\n",
    "    Returns: A list of BeamSearchCandidate objects descended from this node.\n",
    "    \"\"\"\n",
    "    expanded_set = []\n",
    "    feed = {rnn_model.input_symbol: np.array([[self.sequence[-1]]]),\n",
    "            rnn_model.initial_state: self.state,\n",
    "            rnn_model.temperature: temperature,\n",
    "            rnn_model.num_options: num_options}\n",
    "    [predictions, probabilities, new_state] = tf_sess.run(\n",
    "        [rnn_model.output_labels,\n",
    "         rnn_model.normalized_probs,\n",
    "         rnn_model.new_state], feed)\n",
    "    # Get the indices of the num_beams next picks\n",
    "    picks = [predictions[0][x] for x in range(len(predictions[0]))]\n",
    "    for new_char in picks:\n",
    "      new_seq = deepcopy(self.sequence)\n",
    "      new_seq.append(new_char)\n",
    "      expanded_set.append(\n",
    "          BeamSearchCandidate(new_state, new_seq,\n",
    "                              probabilities[0][0][new_char] + self.probability))\n",
    "    return expanded_set\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    return self.sequence == other.sequence\n",
    "\n",
    "  def __ne__(self, other):\n",
    "    return not self.__eq__(other)\n",
    "\n",
    "  def __hash__(self):\n",
    "    return hash(self.sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GfAB0Su-x6l8"
   },
   "outputs": [],
   "source": [
    "def beam_search_generate_sequence(tf_sess, rnn_model, primer, temperature=0.85,\n",
    "                                  termination_condition=None, num_beams=5):\n",
    "  \"\"\"Implements a sequence generator using Beam Search.\n",
    "\n",
    "  Args:\n",
    "    tf_sess: The Tensorflow session containing the rnn_model.\n",
    "    rnn_model: The RNN to use to generate the next element in the sequence.\n",
    "    temperature: Controls how 'Creative' the generated sequence is. Values\n",
    "      close to 0 tend to generate the most likely sequence, while values\n",
    "      closer to 1 generate more original sequences. Acceptable values are\n",
    "      within (0, 1].\n",
    "    termination_condition: A function taking one parameter, a list of\n",
    "      integers, that returns True when a condition is met that signals to the\n",
    "      RNN to return what it has generated so far.\n",
    "    num_beams: The number of possible sequences to keep at each step of the\n",
    "      generation process.\n",
    "\n",
    "  Returns: A list of at most num_beams BeamSearchCandidate objects.\n",
    "  \"\"\"\n",
    "  candidates = []\n",
    "\n",
    "  rnn_current_state = sess.run([rnn_model.initial_state])\n",
    "  #Initialize the state for the primer\n",
    "  for primer_val in primer[:-1]:\n",
    "    feed = {rnn_model.input_symbol: np.array([[primer_val]]),\n",
    "            rnn_model.initial_state: rnn_current_state\n",
    "           }\n",
    "    [rnn_current_state] = tf_sess.run([rnn_model.new_state], feed)\n",
    "\n",
    "  candidates.append(BeamSearchCandidate(rnn_current_state, primer, num_beams))\n",
    "\n",
    "  while True not in [termination_condition(x.sequence) for x in candidates]:\n",
    "    new_candidates = []\n",
    "    for candidate in candidates:\n",
    "      expanded_candidates = candidate.search_from(\n",
    "          tf_sess, rnn_model, temperature, num_beams)\n",
    "      for new in expanded_candidates:\n",
    "        if new not in new_candidates:\n",
    "          #do not reevaluate duplicates\n",
    "          new_candidates.append(new)\n",
    "    candidates = sorted(new_candidates,\n",
    "                        key=lambda x: x.probability, reverse=True)[:num_beams]\n",
    "\n",
    "  return [c for c in candidates if termination_condition(c.sequence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZHIFCwKx6l_"
   },
   "source": [
    "テキスト生成を開始させる最初の文字列と、テキストの長さを指定します。 \n",
    "\n",
    " 「創造性(Creativity)」は、モデルがパターンの一致にどれほど重点を置くかを示しています。出力に繰り返しが見られる場合は、この値を増やしてください。出力がランダムすぎると思ったら場合は、この値を下げてみてください。 \n",
    "\n",
    "結果があまり一般的には見えない場合は、3つの学習セルをもう少し長く実行します。誤差(loss)が下がれば下がるほど、生成テキストは、より厳密に学習データセットに近い結果となるはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-TbhoBcx6l_",
    "outputId": "36aeb3ff-bc6f-4b12-98cf-fdc3a2d9ff33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built LSTM:  2 256 84 1 1 (1, 1)\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt-7270\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "model = RNN(\n",
    "    rnn_num_layers=num_layers,\n",
    "    rnn_state_size=state_size,\n",
    "    num_classes=mapper.size(),\n",
    "    rnn_batch_size=1,\n",
    "    rnn_sequence_length=1)\n",
    "\n",
    "model.build_inference_model()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
    "saver.restore(sess, ckpt)\n",
    "\n",
    "def gen(start_with, pred, creativity):\n",
    "  int_array = mapper.mapstring(start_with)\n",
    "  candidates = beam_search_generate_sequence(\n",
    "      sess, model, int_array, temperature=creativity,\n",
    "      termination_condition=pred,\n",
    "      num_beams=1)\n",
    "  gentext = mapper.maptokens(candidates[0].sequence)\n",
    "  return gentext\n",
    "\n",
    "def lengthlimit(n):\n",
    "  return lambda text: len(text)>n\n",
    "def sentences(n):\n",
    "  return lambda text: mapper.maptokens(text).count(\".\")>=n\n",
    "def paragraph():\n",
    "  return lambda text: mapper.maptokens(text).count(\"\\n\")>0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYabnl2bx6mB",
    "outputId": "850a5c2f-988f-45e5-e61f-e57bae154945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ANTONIO: Who is it ?IE. Do piy min, wher by till blingestn.\n",
      "    Mave in wind for ne cient hafesteres one for yor your yaev yould and londond, afrely,,\n",
      "    At that Wood tho your you, waecth wing Cichbry your,\n",
      "    I theer mien, ward the me a see hen thy yould berliiver to her mesting of rive ore yout,\n",
      "     Thoush F'er in helr all hive und so bing the nost atHer with stourss the madss'\n",
      "    Kom no, you shat with thing in shee and wear lat yom horch.\n",
      "    Aqhat wirte you dose ou? 'con acrerante,\n",
      "    Fow his nast sthe, lost the sarthind.\n",
      "    Read und is liubk.                                                                         Anmen swenely they for musmer, with the woruster. Aurles I I hell with she laikn.\n",
      "    Mush me  tid. Why will nle bes by lothile Vith is entern-that recume thee, anlind on whou to rudes\n",
      "    Bust a hath tull a pnane form gowet proncud, here and thou theat sam,\n",
      "    Tull at the me then with you sowe braen wune\n",
      "    Susralshes to gresheneuy, and tleacenss\n",
      "    thall a the pids my goon and so me els coon fathire\n",
      "    And prove, but anderers\n",
      "    will for gromen hit, suik, in that, food of thee shoundly;\n",
      "    Luttarr my tutt frat yoe?\n",
      "  NUBITORSNO. What whe vay ip withel. Sretels ase lotst a pold,\n",
      "    The shigh old hath not swamure to have\n",
      "    And your for youl cuetin serserby,\n",
      "    And gofe wham wis it whoud thith hesw of in wour.\n",
      "  COLID. For at tall yourd and shaede no hach bethe,\n",
      "    and that maty tood beulder, arld me sende ingee wit te dreike\n",
      "    I to seever four bud so hen? cithrer, lesthitee. worlmy me thave peralion;\n",
      "    Gut be's, hiw woR brafans on now shat blomelf wouk,\n",
      "    Home sithetht\n",
      "    To dutwell wills in wo tege me noptnes?\n",
      "    Whaw so fallers, doed athise sist to mech not on, hither we 'tund, thoun\n",
      "    Noald of wamst is stinle broulk.\n",
      "  HACATIS. Hy weach,\n",
      "    Hithen coluntange bryake, there your wast whave borje ould. I tot at\n",
      "    Sore his the lat apowine i  mest come.\n",
      "  POCKEN. Yich to shaghed as sele the, youd peesiter be, is now rasol.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length_of_generated_text = 2000\n",
    "creativity = 0.85  # Should be greater than 0 but less than 1\n",
    "\n",
    "print(gen(\"  ANTONIO: Who is it ?\", lengthlimit(length_of_generated_text), creativity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HxTTysaKx6mF"
   },
   "source": [
    "\n",
    "### 2. 学習済みのモデルを保存する\n",
    "\n",
    "学習済みのRNNのコピーを保存しおき、後で使えるようにしておきましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "doo9FOHJx6mF",
    "outputId": "fb657df3-3e71-4740-de4d-a627a96a2a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file with ID 1dQEv67yQe10ccsW13sJx89ilhCDmXzxP as shakespeare_seedbank_char-rnn.zip\n"
     ]
    }
   ],
   "source": [
    "save_model_to_drive = False  ## Set this to true to save directly to Google Drive.\n",
    "\n",
    "def save_model_hyperparameters(path):\n",
    "  with open(path, 'w')  as json_file:\n",
    "    model_params = {\n",
    "        'num_layers': model.num_layers,\n",
    "        'state_size': model.state_size,\n",
    "        'num_classes': model.num_classes\n",
    "    }\n",
    "    json.dump(model_params, json_file)\n",
    "\n",
    "def save_to_drive(title, content):\n",
    "  # Install the PyDrive wrapper & import libraries.\n",
    "  !pip install -U -q PyDrive\n",
    "  from pydrive.auth import GoogleAuth\n",
    "  from pydrive.drive import GoogleDrive\n",
    "  from google.colab import auth\n",
    "  from oauth2client.client import GoogleCredentials\n",
    "\n",
    "  # Authenticate and create the PyDrive client.\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "\n",
    "  newfile = drive.CreateFile({'title': title})\n",
    "  newfile.SetContentFile(content)\n",
    "  newfile.Upload()\n",
    "  print('Uploaded file with ID %s as %s'% (newfile.get('id'),\n",
    "         archive_name))\n",
    "    \n",
    "archive_name = ''.join([file_name,'_seedbank_char-rnn.zip'])\n",
    "latest_model = tf.train.latest_checkpoint(CHECKPOINT_DIR).split('/')[2]\n",
    "checkpoints_archive_path = ''.join(['./exports/',archive_name])\n",
    "if not latest_model:\n",
    "  raise ValueError('You must train a model before you can export one.')\n",
    "  \n",
    "%system mkdir exports\n",
    "%rm -f {checkpoints_archive_path}\n",
    "mapper.save(''.join([CHECKPOINT_DIR, 'token_mapping.json']))\n",
    "save_model_hyperparameters(''.join([CHECKPOINT_DIR, 'model_attributes.json']))\n",
    "%system zip '{checkpoints_archive_path}' -@ '{CHECKPOINT_DIR}checkpoint' \\\n",
    "            '{CHECKPOINT_DIR}token_mapping.json' \\\n",
    "            '{CHECKPOINT_DIR}model_attributes.json' \\\n",
    "            '{CHECKPOINT_DIR}{latest_model}.'*\n",
    "\n",
    "if save_model_to_drive:\n",
    "  save_to_drive(archive_name, checkpoints_archive_path)\n",
    "else:\n",
    "  files.download(checkpoints_archive_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CharRNNで文章を生成する",
   "provenance": [],
   "version": "0.3.2"
  },
	"kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
	"accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

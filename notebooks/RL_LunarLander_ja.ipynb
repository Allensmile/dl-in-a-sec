{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "姿勢制御、着陸を学習する",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qI4U4Ft1VU9y"
      },
      "source": [
        "# 姿勢制御、着陸を学習する\n",
        "\n",
        "Stable BaselinesとRL Baselines Zooを用いて、強化学習による姿勢制御、着陸ゲーム(Lunar Lander)の学習を試します。\n",
        "\n",
        "まずStable Baselinesで学習の詳細の流れを確認したあと、CartPoleでも最後に紹介した、RL Baselines Zooのお手軽1行コマンドで学習、結果の可視化を行います。\n",
        "\n",
        "このノートブックは以下の内容を含みます。\n",
        "\n",
        "- 環境準備\n",
        "- Gym環境とエージェントを作成\n",
        "- エージェントの学習と評価\n",
        "- 1行のコマンドで学習\n",
        "- リプレイ動画の生成\n",
        "\n",
        "このノートブックの、未学習エージェント、および学習済みエージェントの振る舞いの観察するまではX分で実行できます。\n",
        "実際にエージェントを学習させ、振る舞いを観察するには、追加でX分程度を要します。\n",
        "\n",
        "\n",
        "なお、GIFアニメーションによる学習前後のプレイの可視化は、ikeyasu氏の[ChainerRL を Colaboratory で動かす - Qiita](https://qiita.com/ikeyasu/items/ec3c88ce13a2d5e41f26) を参考として作成しました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRzAa2hHVVOa",
        "colab_type": "text"
      },
      "source": [
        "# LunarLander-v2\n",
        "\n",
        "LunarLander-v2環境は、離散値で制御します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "um1Wq3L4Vxn6"
      },
      "source": [
        "## A. 環境を準備する\n",
        "\n",
        "Stable Baselinesと依存ライブラリをインストールします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_trhJMuh62N",
        "colab_type": "text"
      },
      "source": [
        "### 1. 必要なライブラリのインストール\n",
        "\n",
        "インストールに5分程度を要します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gWskDE2c9WoN",
        "colab": {}
      },
      "source": [
        "!apt-get -y install swig xvfb python-opengl\n",
        "!pip install box2d box2d-kengz pybullet pyyaml pytablewriter pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "### 2. 必要なPythonライブラリのインポート\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BIedd7Pz9sOs",
        "colab": {}
      },
      "source": [
        "import time, os, gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import A2C, PPO2\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## B. Gym環境を準備する\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pUWGZp3i9wyf",
        "colab": {}
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R5WPDPtWK_a",
        "colab_type": "text"
      },
      "source": [
        "## C. その他の処理を準備する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Ic6e2lXWLCg"
      },
      "source": [
        "### 1. 評価方法の準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4efFdrQ7MBvl"
      },
      "source": [
        "エージェントを評価するためのヘルパー関数を作成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "63M8mSKR-6Zt",
        "colab": {}
      },
      "source": [
        "def evaluate(model, num_steps=1000):\n",
        "  \"\"\"\n",
        "  Evaluate a RL agent\n",
        "  :param model: (BaseRLModel object) the RL Agent\n",
        "  :param num_steps: (int) number of timesteps to evaluate it\n",
        "  :return: (float) Mean reward for the last 100 episodes\n",
        "  \"\"\"\n",
        "  episode_rewards = [0.0]\n",
        "  obs = env.reset()\n",
        "  for i in range(num_steps):\n",
        "      # _states are only useful when using LSTM policies\n",
        "      action, _states = model.predict(obs)\n",
        "      # here, action, rewards and dones are arrays\n",
        "      # because we are using vectorized env\n",
        "      obs, rewards, dones, info = env.step(action)\n",
        "      \n",
        "      # Stats\n",
        "      episode_rewards[-1] += rewards[0]\n",
        "      if dones[0]:\n",
        "          obs = env.reset()\n",
        "          episode_rewards.append(0.0)\n",
        "  # Compute mean reward for the last 100 episodes\n",
        "  mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
        "  print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
        "  \n",
        "  return mean_100ep_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AlC2ufmWZay",
        "colab_type": "text"
      },
      "source": [
        "### 2. プレイ動画の再生用関数\n",
        "\n",
        "次に、仮想ディプレイを利用し、Colaboratory上でエージェントの振る舞いをアニメーションで見られるようにする関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jk5NDLVWS1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def playback(model, env, maxsteps):\n",
        "  # Start virtual display\n",
        "  display = Display(visible=0, size=(1024, 768))\n",
        "  display.start()\n",
        "\n",
        "  os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
        "\n",
        "  frames = []\n",
        "  for i in range(3):\n",
        "      obs = env.reset()\n",
        "      done = False\n",
        "      R = 0\n",
        "      t = 0\n",
        "      while not done and t < maxsteps:\n",
        "          frames.append(env.render(mode = 'rgb_array'))\n",
        "          action, _states = model.predict(obs)        \n",
        "          obs, rewards, dones, info = env.step(action)\n",
        "          R += rewards\n",
        "          t += 1\n",
        "      print('test episode:', i, 'R:', R)\n",
        "  #    model.stop_episode()\n",
        "  #env.render()\n",
        "\n",
        "  return frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBvS357Zh62X",
        "colab_type": "text"
      },
      "source": [
        "## D. モデルの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Op9A2Q7JWGqR"
      },
      "source": [
        "### 1. エージェントの準備\n",
        "\n",
        "A2Cというアルゴリズムで、エージェントを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BnT39NAuWIfS",
        "colab": {}
      },
      "source": [
        "model = A2C(MlpPolicy, env, ent_coef=0.1, verbose=0, tensorboard_log=\"./lunar_tensorboard/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_9F7C9s3WY4i"
      },
      "source": [
        "### 3. 学習前のエージェントの評価\n",
        "\n",
        "実行すると平均の報酬(Mean reward)と、エピソード数(Num episodes)が得られます。学習後の実行と比べてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xDHLMA6NFk95",
        "colab": {}
      },
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, num_steps=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XYStO93rDJe9"
      },
      "source": [
        "### 4. 未学習状態での振る舞いを見る\n",
        "\n",
        "さて、この未学習状態でのプレイぶりを再生してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9d5efJutDMaw",
        "colab": {}
      },
      "source": [
        "frames = playback(model, env, 100)\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "whQ5eSPmxTLk"
      },
      "source": [
        "## E. エージェントを学習させる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2tb-YYCcXAlC"
      },
      "source": [
        "### 1. TensorBoardのセットアップ\n",
        "\n",
        "学習の経過をモニタするため、Tensorboardをセットアップします。セルの出力に、インラインで表示することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3iYFrQVRXDwx",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard.notebook\n",
        "%tensorboard --logdir lunar_tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "### 2. エージェントを学習させる\n",
        "\n",
        "試しに10,000ステップ学習を進めてみます。所要時間は1分程度です。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e4cfSXIB-pTF",
        "colab": {}
      },
      "source": [
        "model.learn(total_timesteps=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T31dZJYNrJwF"
      },
      "source": [
        "### 3. 学習済みエージェントの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K1ExgtyZrIA6",
        "colab": {}
      },
      "source": [
        "#model = A2C.load(\"a2c_lunar\")\n",
        "model = A2C.load(\"rl-baselines-zoo/trained_agents/a2c/LunarLander-v2.pkl\")\n",
        "env = gym.make('LunarLander-v2')\n",
        "# vectorized environments allow to easily multiprocess training\n",
        "# we demonstrate its usefulness in the next examples\n",
        "env = DummyVecEnv([lambda: env]) \n",
        "model.set_env(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dND81wBAh622",
        "colab_type": "text"
      },
      "source": [
        "## F. 学習済みエージェントの評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hM4r1iTayJF",
        "colab_type": "text"
      },
      "source": [
        "### 1. 学習済みエージェントの評価\n",
        "\n",
        "学習後の平均の報酬(Mean reward)と、エピソード数(Num episodes)が得られます。学習前と比べてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ygl_gVmV_QP7",
        "colab": {}
      },
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward = evaluate(model, num_steps=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yS_JXMdgWyiT"
      },
      "source": [
        "### 2. 学習済みエージェントの振る舞いを見る\n",
        "\n",
        "実際の振る舞いを再生してみましょう。学習前より着陸がうまくできているでしょうか。学習前よりは多少改善しているようです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ucazxp0z8xFi",
        "colab": {}
      },
      "source": [
        "frames = playback(model, env, 300)\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgVyQ7QYGBwn",
        "colab_type": "text"
      },
      "source": [
        "### 3. RL Baselines Zooのインストール\n",
        "\n",
        "今回も、ここからは、\n",
        "\n",
        "- 環境を準備する\n",
        "- エージェントを作成する\n",
        "- 環境上で、エージェントに探索させる\n",
        "\n",
        "というステップを1行のコマンドの裏側にまとめてくれるRL Baselines Zooを使ってみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFy_fQZ0E3TQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/araffin/rl-baselines-zoo.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T81a7bbyW2dA"
      },
      "source": [
        "### 4. 1行のコマンドで学習する\n",
        "\n",
        "実行すると、 `logs/a2c/LunarLander-v2_1/` に  `LunarLander-v2.pkl` として、学習済みエージェントのモデルが保存されます。(一般化すると、 `logs/{algorithm}/{env_name}_n` に、 `{env_name}.pkl` として保存されます。)\n",
        "\n",
        "学習の条件は予め最適化されています。今回の場合、16環境を並列させ、秒間1700フレームあまりの高速で学習を進めます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "abH1T7Cgly2C",
        "colab": {}
      },
      "source": [
        "%cd /content/rl-baselines-zoo\n",
        "!python train.py --algo a2c --env LunarLander-v2 --tensorboard-log lunar_tensorboard/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r0chxGuXW8qc"
      },
      "source": [
        "### 5. 学習済みエージェントを読み込み、プレイバック\n",
        "\n",
        "では、学習結果を見てみましょう。学習を複数回実行した場合は、`logs/a2c/LunarLander-v2_1/LunarLander-v2.pkl` の、nの数を変えてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lpCB6zQLDHId",
        "colab": {}
      },
      "source": [
        "%cd /content/rl-baselines-zoo\n",
        "env_id = \"LunarLander-v2\"\n",
        "model_path = \"logs/a2c/LunarLander-v2_1/LunarLander-v2.pkl\"\n",
        "\n",
        "env = gym.make(env_id)\n",
        "env = DummyVecEnv([lambda: env]) \n",
        "model = A2C.load(model_path)\n",
        "model.set_env(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYQXIxPdcRgj",
        "colab_type": "text"
      },
      "source": [
        "次に再生をします。うまく着陸できるようになったでしょうか。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BeAHgZUPDbbr",
        "colab": {}
      },
      "source": [
        "frames = playback(model, env, 500)\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "60hmTQ9dLFDi"
      },
      "source": [
        "# LunarLanderContinuous-v2\n",
        "\n",
        "LunarLanderContinuous-v2環境は、Continuousと名前にある通り、連続値で制御します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiIAf9HBVbQn",
        "colab_type": "text"
      },
      "source": [
        "## A. 環境を準備する\n",
        "\n",
        "LunarLander-v2と共通のため、割愛します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6IskuX8Vb5O",
        "colab_type": "text"
      },
      "source": [
        "## B. Gym環境を準備する\n",
        "\n",
        "今回はRL Baselines Zooでまとめて学習するため、割愛します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8_SftFBVcHQ",
        "colab_type": "text"
      },
      "source": [
        "## C. その他の処理を準備する\n",
        "\n",
        "LunarLander-v2と共通のため、割愛します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQyke2MDV4Dd",
        "colab_type": "text"
      },
      "source": [
        "## D. モデルの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAY2yBpHV8fu",
        "colab_type": "text"
      },
      "source": [
        "今回はRL Baselines Zooでまとめて学習するため、割愛します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6fxV-CkWAoC",
        "colab_type": "text"
      },
      "source": [
        "## E. エージェントを学習させる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_GNekfDXLdvF"
      },
      "source": [
        "### TensorBoardで学習をモニタする\n",
        "\n",
        "再度モニタ用のTensorBoardをセットアップしましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xLh0gcT3LdJ7",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard.notebook\n",
        "%tensorboard --logdir lunar_tensorboard/LunarLanderContinuous-v2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8OBWZgVILi6Y"
      },
      "source": [
        "### 1行のコマンドで学習する\n",
        "\n",
        "RL Baselines Zooをインストール済みであるとして進めます。\n",
        "\n",
        "今回は連続値による制御であるため、PPO2をエージェントのモデルに用います。\n",
        "\n",
        "実行すると、 `logs/ppo2/LunarLanderContinuous-v2_1/` に  `LunarLanderContinuous-v2.pkl` として、学習済みエージェントのモデルが保存されます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "esa-0JwfFkbB",
        "colab": {}
      },
      "source": [
        "%cd /content/rl-baselines-zoo\n",
        "!python train.py --algo ppo2 --env LunarLanderContinuous-v2 --tensorboard-log lunar_tensorboard/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VJFeZk8HBrl",
        "colab_type": "text"
      },
      "source": [
        "### 3. 学習済みエージェントを読み込み、プレイバック\n",
        "\n",
        "では、学習結果を見てみましょう。学習を複数回実行した場合は、`logs/ppo2/LunarLanderContinuous-v2_n/LunarLanderContinuous-v2.pkl` の、nの数を変えてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kfGBaj-nNbSv",
        "colab": {}
      },
      "source": [
        "%cd /content/rl-baselines-zoo\n",
        "env_id = \"LunarLanderContinuous-v2\"\n",
        "model_path = \"logs/ppo2/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.pkl\"\n",
        "\n",
        "model = PPO2.load(model_path)\n",
        "env = gym.make(env_id)\n",
        "env = DummyVecEnv([lambda: env]) \n",
        "model.set_env(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB9Z6O-DdqkN",
        "colab_type": "text"
      },
      "source": [
        "次に再生をします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GD2itZ-iNbaa",
        "colab": {}
      },
      "source": [
        "frames = playback(model, env, 500)\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPfIZLTckYOv",
        "colab_type": "text"
      },
      "source": [
        "## まとめと発展\n",
        "\n",
        "このノートブックでは、LunarLanderを、離散値の制御、連続値の制御2つの異なるパターンで、モデルの種類を変えて習熟させることができました。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RUDdpLvySMNq",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
